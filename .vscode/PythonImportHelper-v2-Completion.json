[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "mediapipe",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mediapipe",
        "description": "mediapipe",
        "detail": "mediapipe",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "jsonify",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "keyboard",
        "importPath": "pynput",
        "description": "pynput",
        "isExtraImport": true,
        "detail": "pynput",
        "documentation": {}
    },
    {
        "label": "keyboard",
        "importPath": "pynput",
        "description": "pynput",
        "isExtraImport": true,
        "detail": "pynput",
        "documentation": {}
    },
    {
        "label": "SVC",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "SVC",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "SVC",
        "importPath": "sklearn.svm",
        "description": "sklearn.svm",
        "isExtraImport": true,
        "detail": "sklearn.svm",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "KMeans",
        "importPath": "sklearn.cluster",
        "description": "sklearn.cluster",
        "isExtraImport": true,
        "detail": "sklearn.cluster",
        "documentation": {}
    },
    {
        "label": "MyGestureClassifier",
        "kind": 6,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "class MyGestureClassifier:\n    def __init__(self):\n        self.samples = []  # list of feature vectors\n        self.labels = []   # corresponding key labels\n        self.model = None\n    def train(self):\n        if not self.samples:\n            print(\"No samples to train on.\")\n            return\n        X = np.array(self.samples)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "fuzzy_distortion",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def fuzzy_distortion(image, strength=5, kernel_size=3):\n    \"\"\"\n    Applies a slight fuzzy distortion to the input image.\n    Simulates small variations in hand/finger appearance.\n    \"\"\"\n    rows, cols = image.shape[:2]\n    dx = np.random.uniform(-strength, strength, size=(rows, cols))\n    dy = np.random.uniform(-strength, strength, size=(rows, cols))\n    map_x, map_y = np.meshgrid(np.arange(cols), np.arange(rows))\n    map_x = (map_x.astype(np.float32) + dx).clip(0, cols - 1)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "save_augmented_sample",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def save_augmented_sample(image, label, count):\n    \"\"\"\n    Saves an augmented (fuzzy-distorted) version of the image to disk.\n    \"\"\"\n    aug_image = fuzzy_distortion(image, strength=5, kernel_size=3)\n    filename = f\"aug_sample_{label}_{count}.jpg\"\n    cv2.imwrite(filename, aug_image)\n    print(f\"Saved augmented sample image: {filename}\")\n# ------------------------\n# HELPER: BEEP",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "beep",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def beep():\n    \"\"\"Plays a short beep sound (using macOS afplay).\"\"\"\n    os.system(\"afplay /System/Library/Sounds/Ping.aiff\")\n# ------------------------\n# HELPER: FLATTEN LANDMARKS\n# ------------------------\ndef flatten_hand_landmarks(hand_landmarks, expected_count=21):\n    if hand_landmarks is None:\n        return [0.0] * (expected_count * 3)\n    vec = []",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "flatten_hand_landmarks",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def flatten_hand_landmarks(hand_landmarks, expected_count=21):\n    if hand_landmarks is None:\n        return [0.0] * (expected_count * 3)\n    vec = []\n    for lm in hand_landmarks.landmark:\n        vec.extend([lm.x, lm.y, lm.z])\n    return vec\ndef flatten_face_landmarks(face_landmarks, expected_count=468):\n    if face_landmarks is None:\n        return [0.0] * (expected_count * 3)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "flatten_face_landmarks",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def flatten_face_landmarks(face_landmarks, expected_count=468):\n    if face_landmarks is None:\n        return [0.0] * (expected_count * 3)\n    vec = []\n    for lm in face_landmarks.landmark:\n        vec.extend([lm.x, lm.y, lm.z])\n    return vec\n# ------------------------\n# FEATURE EXTRACTION FOR TRAINING (Hands, Face, and Pose)\n# ------------------------",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "extract_all_features",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def extract_all_features(hand_results, face_results):\n    \"\"\"\n    Extracts and concatenates hand and face features.\n    (Hand: 2 x 21 x 3; Face: 468 x 3)\n    \"\"\"\n    left_hand_vec = [0.0] * (21 * 3)\n    right_hand_vec = [0.0] * (21 * 3)\n    if hand_results.multi_hand_landmarks:\n        hands = list(hand_results.multi_hand_landmarks)\n        hands = sorted(hands, key=lambda h: h.landmark[mp.solutions.hands.HandLandmark.WRIST].x)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "extract_pose_features",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def extract_pose_features(pose_results, mp_pose):\n    \"\"\"\n    Extracts pose features from selected landmarks (shoulders, elbows, wrists, hips).\n    Returns a flattened vector.\n    \"\"\"\n    if pose_results and pose_results.pose_landmarks:\n        landmarks = [\n            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER,\n            mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.RIGHT_ELBOW,\n            mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "extract_full_features",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def extract_full_features(hand_results, face_results, pose_results, mp_pose):\n    \"\"\"\n    Combines hand, face, and pose features into one feature vector.\n    \"\"\"\n    features1 = extract_all_features(hand_results, face_results)\n    features2 = extract_pose_features(pose_results, mp_pose)\n    return features1 + features2\n# ------------------------\n# FIXED-LENGTH MOTION FEATURES FOR CLUSTERING\n# ------------------------",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "extract_motion_features",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def extract_motion_features(hand_results, face_results, pose_results, mp_pose):\n    \"\"\"\n    Extracts a fixed-length 15-dimensional feature vector for clustering.\n    The vector includes:\n      - Hands: left and right wrist positions (3 coordinates each; if a hand is missing, zeros are used)\n      - Face: nose tip position (3 coordinates; zeros if missing)\n      - Pose: left and right shoulder positions (3 coordinates each; zeros if missing)\n    Total length: 3 + 3 + 3 + 6 = 15.\n    \"\"\"\n    # Hands: fixed length 6",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "initialize_classifier",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def initialize_classifier():\n    return MyGestureClassifier()\ndef update_classifier(classifier, samples):\n    classifier.update(samples)\n    return classifier\n# ------------------------\n# PERFORMANCE METRICS HELPERS (for research logging)\n# ------------------------\ndef update_performance_metrics(actual, predicted, confidence, timestamp):\n    global performance_metrics",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "update_classifier",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def update_classifier(classifier, samples):\n    classifier.update(samples)\n    return classifier\n# ------------------------\n# PERFORMANCE METRICS HELPERS (for research logging)\n# ------------------------\ndef update_performance_metrics(actual, predicted, confidence, timestamp):\n    global performance_metrics\n    performance_metrics[\"total_predictions\"] += 1\n    correct = (actual == predicted)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "update_performance_metrics",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def update_performance_metrics(actual, predicted, confidence, timestamp):\n    global performance_metrics\n    performance_metrics[\"total_predictions\"] += 1\n    correct = (actual == predicted)\n    if correct:\n        performance_metrics[\"correct_predictions\"] += 1\n    performance_metrics[\"prediction_history\"].append({\n        \"timestamp\": timestamp,\n        \"actual\": actual,\n        \"predicted\": predicted,",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "compute_training_accuracy",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def compute_training_accuracy(classifier):\n    if classifier.model is None or not classifier.samples:\n        return 0.0\n    X = np.array(classifier.samples)\n    y = np.array(classifier.labels)\n    preds = classifier.model.predict(X)\n    correct = sum(1 for p, a in zip(preds, y) if p == a)\n    return correct / len(y)\ndef save_performance_metrics():\n    with open(\"performance_metrics.json\", \"w\") as f:",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "save_performance_metrics",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def save_performance_metrics():\n    with open(\"performance_metrics.json\", \"w\") as f:\n        json.dump(performance_metrics, f, indent=4)\n    print(\"Performance metrics saved.\")\n# ------------------------\n# SYSTEM ACTION EXECUTION\n# ------------------------\ndef execute_system_action(action):\n    print(f\"Executing system action: {action}\")\n    if action == \"volume_up\":",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "execute_system_action",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def execute_system_action(action):\n    print(f\"Executing system action: {action}\")\n    if action == \"volume_up\":\n        os.system(\"osascript -e 'set volume output volume ((output volume of (get volume settings)) + 10)'\")\n    elif action == \"volume_down\":\n        os.system(\"osascript -e 'set volume output volume ((output volume of (get volume settings)) - 10)'\")\n    elif action == \"next_song\":\n        print(\"Action: Next Song executed.\")\n    elif action == \"pause\":\n        print(\"Action: Pause executed.\")",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "draw_hand_and_face_info",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def draw_hand_and_face_info(frame, hand_results, face_results):\n    h, w, _ = frame.shape\n    # Draw hand landmarks (each hand in red circles)\n    if hand_results.multi_hand_landmarks:\n        for hand_landmarks in hand_results.multi_hand_landmarks:\n            for lm in hand_landmarks.landmark:\n                cx, cy = int(lm.x * w), int(lm.y * h)\n                cv2.circle(frame, (cx, cy), 2, (0, 0, 255), -1)\n    # Draw face landmarks (in green)\n    if face_results.multi_face_landmarks:",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "draw_pose_info",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def draw_pose_info(frame, pose_results, mp_pose):\n    if pose_results.pose_landmarks:\n        h, w, _ = frame.shape\n        landmarks_to_draw = [\n            mp_pose.PoseLandmark.LEFT_SHOULDER,\n            mp_pose.PoseLandmark.RIGHT_SHOULDER,\n            mp_pose.PoseLandmark.LEFT_ELBOW,\n            mp_pose.PoseLandmark.RIGHT_ELBOW,\n            mp_pose.PoseLandmark.LEFT_WRIST,\n            mp_pose.PoseLandmark.RIGHT_WRIST,",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "draw_debug_info",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def draw_debug_info(frame, hand_results, face_results, pose_results, state, mp_pose=None):\n    h, w, _ = frame.shape\n    cv2.putText(frame, f\"Mode: {state['mode']}\", (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n    cv2.putText(frame, f\"Predicted: {state['predicted_action']} ({state['confidence']:.2f})\", (10, 70),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n    # Draw hand and face landmarks.\n    frame = draw_hand_and_face_info(frame, hand_results, face_results)\n    # Draw pose landmarks.\n    if pose_results and mp_pose is not None:",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "on_key_press",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def on_key_press(key):\n    try:\n        key_name = key.char if hasattr(key, 'char') and key.char is not None else str(key)\n        if key_name in KEY_ACTIONS:\n            timestamp = time.time()\n            key_press_buffer.append((key_name, timestamp))\n            print(f\"Captured key press: {key_name} at {timestamp}\")\n    except Exception as e:\n        print(\"Error capturing key press:\", e)\nkeyboard_listener = keyboard.Listener(on_press=on_key_press)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "cluster_user_movements",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def cluster_user_movements(motion_data):\n    if len(motion_data) < 10:\n        print(\"Not enough data for clustering.\")\n        return None\n    X = np.array(motion_data)\n    kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n    labels = kmeans.fit_predict(X)\n    print(\"User movement clusters:\", labels)\n    return labels\n# ------------------------",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def index():\n    return render_template('index.html', state=shared_state)\n@app.route('/toggle_mode', methods=['POST'])\ndef toggle_mode():\n    new_mode = request.form.get(\"mode\")\n    shared_state[\"mode\"] = new_mode\n    print(\"Switched mode to\", new_mode)\n    return jsonify(success=True)\n@app.route('/confirm_action', methods=['POST'])\ndef confirm_action():",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "toggle_mode",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def toggle_mode():\n    new_mode = request.form.get(\"mode\")\n    shared_state[\"mode\"] = new_mode\n    print(\"Switched mode to\", new_mode)\n    return jsonify(success=True)\n@app.route('/confirm_action', methods=['POST'])\ndef confirm_action():\n    action = shared_state.get(\"predicted_action\")\n    execute_system_action(action)\n    return jsonify(success=True)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "confirm_action",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def confirm_action():\n    action = shared_state.get(\"predicted_action\")\n    execute_system_action(action)\n    return jsonify(success=True)\ndef run_flask():\n    app.run(debug=False, use_reloader=False)\n# ------------------------\n# MAIN APPLICATION LOOP\n# ------------------------\ndef main():",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "run_flask",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def run_flask():\n    app.run(debug=False, use_reloader=False)\n# ------------------------\n# MAIN APPLICATION LOOP\n# ------------------------\ndef main():\n    # Start the Flask server in a separate thread.\n    flask_thread = threading.Thread(target=run_flask)\n    flask_thread.daemon = True\n    flask_thread.start()",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "def main():\n    # Start the Flask server in a separate thread.\n    flask_thread = threading.Thread(target=run_flask)\n    flask_thread.daemon = True\n    flask_thread.start()\n    # Open the webcam.\n    cap = cv2.VideoCapture(0, cv2.CAP_AVFOUNDATION)\n    if not cap.isOpened():\n        print(\"Error: Unable to open webcam.\")\n        return",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENCV_AVFOUNDATION_IGNORE_CONTINUITY\"]",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "os.environ[\"OPENCV_AVFOUNDATION_IGNORE_CONTINUITY\"] = \"1\"\nimport cv2\nimport mediapipe as mp\nimport numpy as np\nimport time\nimport threading\nimport pickle\nimport random\nimport json\nfrom collections import deque",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "shared_state",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "shared_state = {\n    \"mode\": \"production\",  # \"training\" or \"production\"\n    \"predicted_action\": \"none\",\n    \"confidence\": 0.0\n}\n# Frame buffer: stores the last 5 seconds of frames (assumed FPS = 30)\nBUFFER_SECONDS = 5\nFPS = 30\nFRAME_BUFFER_SIZE = BUFFER_SECONDS * FPS\nframe_buffer = deque(maxlen=FRAME_BUFFER_SIZE)  # Each entry: (frame, timestamp)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "BUFFER_SECONDS",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "BUFFER_SECONDS = 5\nFPS = 30\nFRAME_BUFFER_SIZE = BUFFER_SECONDS * FPS\nframe_buffer = deque(maxlen=FRAME_BUFFER_SIZE)  # Each entry: (frame, timestamp)\n# Key press events: each entry is a tuple (key_name, timestamp)\nkey_press_buffer = []\n# Training samples: list of tuples (feature_vector, key_label)\ntraining_samples = []\n# Temporary buffer for samples collected in current cycle.\nsamples_buffer = []",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "FPS",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "FPS = 30\nFRAME_BUFFER_SIZE = BUFFER_SECONDS * FPS\nframe_buffer = deque(maxlen=FRAME_BUFFER_SIZE)  # Each entry: (frame, timestamp)\n# Key press events: each entry is a tuple (key_name, timestamp)\nkey_press_buffer = []\n# Training samples: list of tuples (feature_vector, key_label)\ntraining_samples = []\n# Temporary buffer for samples collected in current cycle.\nsamples_buffer = []\n# Motion data for clustering (for research)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "FRAME_BUFFER_SIZE",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "FRAME_BUFFER_SIZE = BUFFER_SECONDS * FPS\nframe_buffer = deque(maxlen=FRAME_BUFFER_SIZE)  # Each entry: (frame, timestamp)\n# Key press events: each entry is a tuple (key_name, timestamp)\nkey_press_buffer = []\n# Training samples: list of tuples (feature_vector, key_label)\ntraining_samples = []\n# Temporary buffer for samples collected in current cycle.\nsamples_buffer = []\n# Motion data for clustering (for research)\nmotion_data = []",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "frame_buffer",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "frame_buffer = deque(maxlen=FRAME_BUFFER_SIZE)  # Each entry: (frame, timestamp)\n# Key press events: each entry is a tuple (key_name, timestamp)\nkey_press_buffer = []\n# Training samples: list of tuples (feature_vector, key_label)\ntraining_samples = []\n# Temporary buffer for samples collected in current cycle.\nsamples_buffer = []\n# Motion data for clustering (for research)\nmotion_data = []\n# Performance metrics for logging (for research)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "key_press_buffer",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "key_press_buffer = []\n# Training samples: list of tuples (feature_vector, key_label)\ntraining_samples = []\n# Temporary buffer for samples collected in current cycle.\nsamples_buffer = []\n# Motion data for clustering (for research)\nmotion_data = []\n# Performance metrics for logging (for research)\nperformance_metrics = {\n    \"total_predictions\": 0,",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "training_samples",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "training_samples = []\n# Temporary buffer for samples collected in current cycle.\nsamples_buffer = []\n# Motion data for clustering (for research)\nmotion_data = []\n# Performance metrics for logging (for research)\nperformance_metrics = {\n    \"total_predictions\": 0,\n    \"correct_predictions\": 0,\n    \"prediction_history\": []  # Each entry: {timestamp, actual, predicted, confidence, correct}",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "samples_buffer",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "samples_buffer = []\n# Motion data for clustering (for research)\nmotion_data = []\n# Performance metrics for logging (for research)\nperformance_metrics = {\n    \"total_predictions\": 0,\n    \"correct_predictions\": 0,\n    \"prediction_history\": []  # Each entry: {timestamp, actual, predicted, confidence, correct}\n}\n# ------------------------",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "motion_data",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "motion_data = []\n# Performance metrics for logging (for research)\nperformance_metrics = {\n    \"total_predictions\": 0,\n    \"correct_predictions\": 0,\n    \"prediction_history\": []  # Each entry: {timestamp, actual, predicted, confidence, correct}\n}\n# ------------------------\n# CONFIGURABLE CONSTANTS\n# ------------------------",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "performance_metrics",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "performance_metrics = {\n    \"total_predictions\": 0,\n    \"correct_predictions\": 0,\n    \"prediction_history\": []  # Each entry: {timestamp, actual, predicted, confidence, correct}\n}\n# ------------------------\n# CONFIGURABLE CONSTANTS\n# ------------------------\nSAMPLE_THRESHOLD = 10         # Number of samples to collect before updating the classifier\nCONFIDENCE_THRESHOLD = 0.7    # Confidence threshold for production suggestions / auto-act",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "SAMPLE_THRESHOLD",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "SAMPLE_THRESHOLD = 10         # Number of samples to collect before updating the classifier\nCONFIDENCE_THRESHOLD = 0.7    # Confidence threshold for production suggestions / auto-act\nMOTION_WINDOW = 3             # Number of consecutive frames to aggregate for a prediction\n# Designated keys for key-based training and actions.\nKEY_ACTIONS = {\n    \"volume_up\": \"volume_up\",\n    \"volume_down\": \"volume_down\",\n    \"next_song\": \"next_song\",\n    \"pause\": \"pause\"\n}",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "CONFIDENCE_THRESHOLD",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "CONFIDENCE_THRESHOLD = 0.7    # Confidence threshold for production suggestions / auto-act\nMOTION_WINDOW = 3             # Number of consecutive frames to aggregate for a prediction\n# Designated keys for key-based training and actions.\nKEY_ACTIONS = {\n    \"volume_up\": \"volume_up\",\n    \"volume_down\": \"volume_down\",\n    \"next_song\": \"next_song\",\n    \"pause\": \"pause\"\n}\n# ------------------------",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "MOTION_WINDOW",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "MOTION_WINDOW = 3             # Number of consecutive frames to aggregate for a prediction\n# Designated keys for key-based training and actions.\nKEY_ACTIONS = {\n    \"volume_up\": \"volume_up\",\n    \"volume_down\": \"volume_down\",\n    \"next_song\": \"next_song\",\n    \"pause\": \"pause\"\n}\n# ------------------------\n# DATA AUGMENTATION: FUZZY DISTORTION",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "KEY_ACTIONS",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "KEY_ACTIONS = {\n    \"volume_up\": \"volume_up\",\n    \"volume_down\": \"volume_down\",\n    \"next_song\": \"next_song\",\n    \"pause\": \"pause\"\n}\n# ------------------------\n# DATA AUGMENTATION: FUZZY DISTORTION\n# ------------------------\ndef fuzzy_distortion(image, strength=5, kernel_size=3):",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "keyboard_listener",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "keyboard_listener = keyboard.Listener(on_press=on_key_press)\nkeyboard_listener.daemon = True\nkeyboard_listener.start()\n# ------------------------\n# OPTIONAL: CLUSTERING OF USER MOTION (for research)\n# ------------------------\ndef cluster_user_movements(motion_data):\n    if len(motion_data) < 10:\n        print(\"Not enough data for clustering.\")\n        return None",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "keyboard_listener.daemon",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "keyboard_listener.daemon = True\nkeyboard_listener.start()\n# ------------------------\n# OPTIONAL: CLUSTERING OF USER MOTION (for research)\n# ------------------------\ndef cluster_user_movements(motion_data):\n    if len(motion_data) < 10:\n        print(\"Not enough data for clustering.\")\n        return None\n    X = np.array(motion_data)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "integrated_gesture_pose_fuzzy",
        "description": "integrated_gesture_pose_fuzzy",
        "peekOfCode": "app = Flask(__name__, template_folder='templates')\n@app.route('/')\ndef index():\n    return render_template('index.html', state=shared_state)\n@app.route('/toggle_mode', methods=['POST'])\ndef toggle_mode():\n    new_mode = request.form.get(\"mode\")\n    shared_state[\"mode\"] = new_mode\n    print(\"Switched mode to\", new_mode)\n    return jsonify(success=True)",
        "detail": "integrated_gesture_pose_fuzzy",
        "documentation": {}
    },
    {
        "label": "MyGestureClassifier",
        "kind": 6,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "class MyGestureClassifier:\n    def __init__(self):\n        self.samples = []\n        self.labels = []\n        self.model = None\n    def train(self):\n        if not self.samples:\n            print(\"No samples to train on.\")\n            return\n        X = np.array(self.samples)",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "fuzzy_distortion",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def fuzzy_distortion(image, strength=5, kernel_size=3):\n    \"\"\"Applies slight random displacement and blur to simulate variability.\"\"\"\n    rows, cols = image.shape[:2]\n    dx = np.random.uniform(-strength, strength, size=(rows, cols))\n    dy = np.random.uniform(-strength, strength, size=(rows, cols))\n    map_x, map_y = np.meshgrid(np.arange(cols), np.arange(rows))\n    map_x = (map_x.astype(np.float32) + dx).clip(0, cols - 1)\n    map_y = (map_y.astype(np.float32) + dy).clip(0, rows - 1)\n    distorted = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR)\n    if kernel_size > 1:",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "save_augmented_sample",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def save_augmented_sample(image, label, count):\n    \"\"\"Saves a fuzzy-distorted image to disk.\"\"\"\n    aug_image = fuzzy_distortion(image, strength=5, kernel_size=3)\n    filename = f\"aug_sample_{label}_{count}.jpg\"\n    cv2.imwrite(filename, aug_image)\n    print(f\"Saved augmented sample: {filename}\")\n# ------------------------\n# HELPER: BEEP\n# ------------------------\ndef beep():",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "beep",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def beep():\n    os.system(\"afplay /System/Library/Sounds/Ping.aiff\")\n# ------------------------\n# HELPER: FLATTEN LANDMARKS\n# ------------------------\ndef flatten_hand_landmarks(hand_landmarks, expected_count=21):\n    if hand_landmarks is None:\n        return [0.0]*(expected_count*3)\n    vec = []\n    for lm in hand_landmarks.landmark:",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "flatten_hand_landmarks",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def flatten_hand_landmarks(hand_landmarks, expected_count=21):\n    if hand_landmarks is None:\n        return [0.0]*(expected_count*3)\n    vec = []\n    for lm in hand_landmarks.landmark:\n        vec.extend([lm.x, lm.y, lm.z])\n    return vec\ndef flatten_face_landmarks(face_landmarks, expected_count=468):\n    if face_landmarks is None:\n        return [0.0]*(expected_count*3)",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "flatten_face_landmarks",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def flatten_face_landmarks(face_landmarks, expected_count=468):\n    if face_landmarks is None:\n        return [0.0]*(expected_count*3)\n    vec = []\n    for lm in face_landmarks.landmark:\n        vec.extend([lm.x, lm.y, lm.z])\n    return vec\n# ------------------------\n# FEATURE EXTRACTION (Hands, Face, Pose)\n# ------------------------",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "extract_all_features",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def extract_all_features(hand_results, face_results):\n    \"\"\"Concatenates hand (2x21x3) and face (468x3) features.\"\"\"\n    left_hand = [0.0]*(21*3)\n    right_hand = [0.0]*(21*3)\n    if hand_results.multi_hand_landmarks:\n        hands = list(hand_results.multi_hand_landmarks)\n        hands = sorted(hands, key=lambda h: h.landmark[mp.solutions.hands.HandLandmark.WRIST].x)\n        if len(hands) >= 2:\n            left_hand = flatten_hand_landmarks(hands[0])\n            right_hand = flatten_hand_landmarks(hands[1])",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "extract_pose_features",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def extract_pose_features(pose_results, mp_pose):\n    \"\"\"Extracts pose features from eight key landmarks.\"\"\"\n    if pose_results and pose_results.pose_landmarks:\n        landmarks = [\n            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER,\n            mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.RIGHT_ELBOW,\n            mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,\n            mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP\n        ]\n        feat = []",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "extract_full_features",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def extract_full_features(hand_results, face_results, pose_results, mp_pose):\n    \"\"\"Combines hand, face, and pose features into one vector.\"\"\"\n    return extract_all_features(hand_results, face_results) + extract_pose_features(pose_results, mp_pose)\n# ------------------------\n# FIXED-LENGTH MOTION FEATURES (for activity_model clustering)\n# ------------------------\ndef extract_motion_features(hand_results, face_results, pose_results, mp_pose):\n    \"\"\"\n    Returns a 15-dimensional vector:\n      - Hands: left and right wrist positions (3 coords each)",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "extract_motion_features",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def extract_motion_features(hand_results, face_results, pose_results, mp_pose):\n    \"\"\"\n    Returns a 15-dimensional vector:\n      - Hands: left and right wrist positions (3 coords each)\n      - Face: nose tip (3 coords)\n      - Pose: left and right shoulders (3 coords each)\n    \"\"\"\n    left_wrist = [0.0, 0.0, 0.0]\n    right_wrist = [0.0, 0.0, 0.0]\n    if hand_results.multi_hand_landmarks:",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "initialize_classifier",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def initialize_classifier():\n    return MyGestureClassifier()\ndef update_classifier(classifier, samples):\n    classifier.update(samples)\n    return classifier\n# ------------------------\n# PERFORMANCE METRICS HELPERS\n# ------------------------\ndef update_performance_metrics(actual, predicted, confidence, timestamp):\n    global performance_metrics",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "update_classifier",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def update_classifier(classifier, samples):\n    classifier.update(samples)\n    return classifier\n# ------------------------\n# PERFORMANCE METRICS HELPERS\n# ------------------------\ndef update_performance_metrics(actual, predicted, confidence, timestamp):\n    global performance_metrics\n    performance_metrics[\"total_predictions\"] += 1\n    correct = (actual == predicted)",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "update_performance_metrics",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def update_performance_metrics(actual, predicted, confidence, timestamp):\n    global performance_metrics\n    performance_metrics[\"total_predictions\"] += 1\n    correct = (actual == predicted)\n    if correct:\n        performance_metrics[\"correct_predictions\"] += 1\n    performance_metrics[\"prediction_history\"].append({\n        \"timestamp\": timestamp,\n        \"actual\": actual,\n        \"predicted\": predicted,",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "compute_training_accuracy",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def compute_training_accuracy(classifier):\n    if classifier.model is None or not classifier.samples:\n        return 0.0\n    X = np.array(classifier.samples)\n    y = np.array(classifier.labels)\n    preds = classifier.model.predict(X)\n    correct = sum(1 for p, a in zip(preds, y) if p == a)\n    return correct / len(y)\ndef save_performance_metrics():\n    with open(\"performance_metrics.json\", \"w\") as f:",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "save_performance_metrics",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def save_performance_metrics():\n    with open(\"performance_metrics.json\", \"w\") as f:\n        json.dump(performance_metrics, f, indent=4)\n    print(\"Performance metrics saved.\")\n# ------------------------\n# SYSTEM ACTION EXECUTION\n# ------------------------\ndef execute_system_action(action):\n    print(f\"Executing system action: {action}\")\n    if action == \"volume_up\":",
        "detail": "integrated_gesture_pose_fuzzy_andmotion",
        "documentation": {}
    },
    {
        "label": "execute_system_action",
        "kind": 2,
        "importPath": "integrated_gesture_pose_fuzzy_andmotion",
        "description": "integrated_gesture_pose_fuzzy_andmotion",
        "peekOfCode": "def execute_system_action(action):\n    print(f\"Executing system action: {action}\")\n    if action == \"volume_up\":\n        os.system(\"osascript -e 'set volume output volume ((output volume of (get volume settings)) + 10)'\")\n    elif action == \"volume_down\":\n        os.system(\"osascript -e 'set volume output volume ((output volume of (get volume settings)) - 10)'\")\n    elif action == \"next_song\":\n        print(\"Action: Next Song executed.\")\n    elif action == \"pause\":\n        print(\"Action: Pause executed.\")",
        "detail": "integrated_gesture_pose_fu